{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d830b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain langchain-ollama langchain-text-splitters langchain-chroma langchain-community gradio chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ad071f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luciaderulos/.pyenv/versions/3.11.7/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importamos todo lo que vamos a necesitar\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import tempfile\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b04407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logging\n",
    "logging.basicConfig(    \n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ceb1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos los parámetros de Ollama\n",
    "\n",
    "PERSISTENCE_DIR = \"chroma_db\"\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 100\n",
    "PDF_URLS = [\n",
    "    \"https://www.sanidad.gob.es/areas/saludDigital/doc/eIASNS_v13.pdf\",\n",
    "    \"https://www.unespa.es/main-files/uploads/2020/01/El-seguro-del-hogar-y-los-gremios-de-reparadores-FINAL.pdf\",\n",
    "\n",
    "]\n",
    "\n",
    "LLM_MODEL = \"gemma3:270m\"  # Reemplaza con el nombre de tu modelo Ollama\n",
    "EMBEDDING_MODEL = \"embeddinggemma:300m\"  # Reemplaza con el nombre de tu modelo de embeddings Ollama\n",
    "TEMPERATURE = 0.1 #temperatura baja, determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7452678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clase rag \n",
    "\n",
    "class RAGSystem:\n",
    "    \n",
    "    def __init__(self, pdf_urls: List[str], persistence_dir: str = PERSISTENCE_DIR):\n",
    "        \n",
    "        self.pdf_urls = pdf_urls\n",
    "        self.persistence_dir = persistence_dir\n",
    "        self.documents = []\n",
    "        self.vector_store = None\n",
    "        self.llm = None\n",
    "        self.chain = None\n",
    "\n",
    "        callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "        self.llm = ChatOllama(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            callback_manager=callback_manager            \n",
    "        )\n",
    "        \n",
    "        self.embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)        \n",
    "        logger.info(f\"Initialized RAG system with {len(pdf_urls)} PDFs\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc00d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(self):\n",
    "    logger.info(\"Loading documents from PDFs...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=CHUNK_SIZE,\n",
    "            chunk_overlap=CHUNK_OVERLAP,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \".\"]\n",
    "    )\n",
    "\n",
    "    all_pages = []\n",
    "    for url in self.pdf_urls:\n",
    "        try:\n",
    "            loader = PyPDFLoader(url)\n",
    "            pages = loader.load()\n",
    "            logger.info(f\"Loaded {len(pages)} pages from {url}\")\n",
    "            all_pages.extend(pages)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {url}: {e}\")\n",
    "\n",
    "    # divimos en chunks        \n",
    "    self.documents = text_splitter.split_documents(all_pages)\n",
    "    logger.info(f\"Split documents into {len(self.documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34538603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(self) -> None:\n",
    "    \n",
    "   #borramos la carpeta de persistencia si existe \n",
    "    if os.path.exists(self.persistence_dir):\n",
    "        import shutil\n",
    "        logger.info(f\"Removing existing persistence directory at {self.persistence_dir}\")\n",
    "        shutil.rmtree(self.persistence_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "    # Creamos el vector store\n",
    "    logger.info(\"Creating vector store...\")\n",
    "    if not self.documents:\n",
    "      self.load_documents()\n",
    "\n",
    "    #creamos la carpeta de persistencia si no existe\n",
    "    if not os.path.exists(self.persistence_dir):\n",
    "        os.makedirs(self.persistence_dir)\n",
    "\n",
    "    #creamos el vector store con manejo de errores\n",
    "    try:\n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=self.documents,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=self.persistence_dir\n",
    "        )\n",
    "\n",
    "        \n",
    "        logger.info(f\"Vectorstore creada satisfactoriamente con {len(self.documents)} documentos\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating vector store: {e}\")\n",
    "        raise\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6b5cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chain(self):\n",
    "\n",
    "    if not self.vector_store:\n",
    "        self.create_vectorstore()\n",
    "\n",
    "    #creamos el retriever\n",
    "    retriever = self.vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    # Definimos el prompt template\n",
    "    template = \"\"\"\n",
    "    Eres un asistente útil que ayuda a responder preguntas basadas en los siguientes documentos proporcionados de forma educada y profesional.\n",
    "    Utiliza la información de los documentos para formular una respuesta precisa y completa.               \n",
    "    \n",
    "    (1) Se atento al detalle y asegúrate de que la respuesta sea relevante para la pregunta realizada.\n",
    "    (2) Si la información no está disponible en los documentos, responde informando que los documentos no contienen la información solicitada.\n",
    "    (3) Si el contexto lo permite, responde de forma detalla y precisa.\n",
    "    (4) No inventes información ni hagas suposiciones; limita tus respuestas únicamente a la información proporcionada en los documentos.\n",
    "    (5) Revisa tu respuesta antes de enviarla para asegurarte de que sea clara y coherente, y responda a la pregunta realizada.\n",
    "    (6) Debajo de la respuesta, incluye una sección titulada \"Fuentes\" donde enumeres las fuentes de la información utilizada en la respuesta.\n",
    "\n",
    "    Piensa paso a paso y proporciona la respuesta en español.\n",
    "\n",
    "    Utiliza el siguiente formato para estructurar tu respuesta:\n",
    "    ### Pregunta: {question}###\n",
    "    ### Respuesta: {context} ###\n",
    "    ### Fuentes:  ###\n",
    "     \n",
    "    \"\"\"  \n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Configuramos la cadena de RAG\n",
    "    self.chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | self.llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    logger.info(\"RAG cadena configurada correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81d01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método para responder preguntas\n",
    "def answer_question(self, question: str) -> str:\n",
    "\n",
    "    if not self.chain:\n",
    "        self.setup_chain()\n",
    "\n",
    "    logger.info(f\"Respondiendo pregunta: {question}\")   \n",
    "    try:\n",
    "        answer = self.chain.invoke(question)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error answering question: {e}\")\n",
    "        return \"Lo siento, ha ocurrido un error al procesar tu pregunta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d99bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interfaz gradio\n",
    "def create_gradio_interface(rag_system: RAGSystem) -> gr.Interface:\n",
    "   \n",
    "    def get_answer(question: str) -> str:\n",
    "        return rag_system.answer_question(question)\n",
    "    \n",
    "    # Definimos la interfaz de Gradio\n",
    "    interface = gr.Interface(\n",
    "        fn=get_answer,\n",
    "        inputs=gr.Textbox(\n",
    "            label=\"Tu Pregunta\",\n",
    "            placeholder=\"Escribe tu pregunta aquí...\"),\n",
    "        outputs=gr.Markdown(label=\"Respuesta\"),\n",
    "        title=\"Sistema RAG con Ollama y LangChain\",\n",
    "        description=\"Haz preguntas basadas en los documentos PDF cargados.\",\n",
    "        theme=gr.themes.Soft(),\n",
    "\n",
    "        # ejemplos de preguntas\n",
    "        examples=[\n",
    "            \"¿Cuáles son las principales características de la IA en sanidad?\",\n",
    "            \"¿Qué recomendaciones se dan para el seguro del hogar?\",\n",
    "            \"¿Cómo afecta la digitalización a los servicios de salud?\",\n",
    "            \"¿Qué medidas de seguridad se sugieren para la protección de datos en sanidad?\"\n",
    "        ]\n",
    "    )\n",
    "    return interface\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94bd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "\n",
    "    try:\n",
    "\n",
    "        print(\"Comprobando Ollama Models disponibles...\")\n",
    "        try:\n",
    "            import requests\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "            print(\"Ollama Models disponibles:\")\n",
    "            if response.status_code == 200:\n",
    "                for model in response.json().get(\"models\", []):\n",
    "                    print(f\"- {model['name']}\")\n",
    "            else:\n",
    "                print(f\"Error al obtener los modelos: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"No se pudo conectar a Ollama API: {e}\")\n",
    "\n",
    "        print(f\"Usando modelo: {LLM_MODEL}\")\n",
    "        print(f\"y modelo de embedding: {EMBEDDING_MODEL}\")\n",
    "        print(\"Inicializando sistema RAG...\")\n",
    "\n",
    "        #creamos el sistema RAG\n",
    "        rag_system = RAGSystem(pdf_urls=PDF_URLS)\n",
    "\n",
    "        #cargamos los documentos y creamos el vector store\n",
    "        rag_system.load_documents()\n",
    "        rag_system.create_vectorstore()\n",
    "\n",
    "        #test con pregunta de control\n",
    "        logger.info(\"Realizando pregunta de control...\")\n",
    "        test_answer= rag_system.answer_question(\"¿Cuáles son las principales características de la IA en sanidad?\")\n",
    "        logger.info(f\"Respuesta de control recibida (tamaño: {len(test_answer)})\")\n",
    "\n",
    "        #creamos la interfaz de gradio\n",
    "        logger.info(\"Creando interfaz de Gradio...\")\n",
    "        gr_interface = create_gradio_interface(rag_system)\n",
    "        gr_interface.launch(share=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la ejecución principal: {e}\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7ef3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# añadir los métodos a la clase RAGSystem\n",
    "RAGSystem.load_documents = load_documents\n",
    "RAGSystem.create_vectorstore = create_vectorstore   \n",
    "RAGSystem.setup_chain = setup_chain\n",
    "RAGSystem.answer_question = answer_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d0eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprobando Ollama Models disponibles...\n",
      "Ollama Models disponibles:\n",
      "- gemma3:270m\n",
      "- embeddinggemma:300m\n",
      "- gemma3:1b\n",
      "Usando modelo: gemma3:270m\n",
      "y modelo de embedding: embeddinggemma:300m\n",
      "Inicializando sistema RAG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:49:08,244 - __main__ - INFO - Initialized RAG system with 2 PDFs\n",
      "2026-01-21 21:49:08,246 - __main__ - INFO - Loading documents from PDFs...\n",
      "2026-01-21 21:49:23,921 - __main__ - INFO - Loaded 45 pages from https://www.sanidad.gob.es/areas/saludDigital/doc/eIASNS_v13.pdf\n",
      "2026-01-21 21:49:28,486 - __main__ - INFO - Loaded 22 pages from https://www.unespa.es/main-files/uploads/2020/01/El-seguro-del-hogar-y-los-gremios-de-reparadores-FINAL.pdf\n",
      "2026-01-21 21:49:28,512 - __main__ - INFO - Split documents into 376 chunks\n",
      "2026-01-21 21:49:28,515 - __main__ - INFO - Removing existing persistence directory at chroma_db\n",
      "2026-01-21 21:49:28,518 - __main__ - INFO - Creating vector store...\n",
      "2026-01-21 21:49:28,587 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2026-01-21 21:55:28,768 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:55:29,871 - __main__ - INFO - Vectorstore creada satisfactoriamente con 376 documentos\n",
      "2026-01-21 21:55:29,873 - __main__ - INFO - Realizando pregunta de control...\n",
      "2026-01-21 21:55:29,876 - __main__ - INFO - RAG cadena configurada correctamente.\n",
      "2026-01-21 21:55:29,878 - __main__ - INFO - Respondiendo pregunta: ¿Cuáles son las principales características de la IA en sanidad?\n",
      "2026-01-21 21:55:30,331 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:55:42,625 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:56:19,686 - __main__ - INFO - Respuesta de control recibida (tamaño: 2122)\n",
      "2026-01-21 21:56:19,687 - __main__ - INFO - Creando interfaz de Gradio...\n",
      "/Users/luciaderulos/.pyenv/versions/3.11.7/lib/python3.11/site-packages/gradio/interface.py:171: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  super().__init__(\n",
      "2026-01-21 21:56:20,199 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/https%3A/api.gradio.app/gradio-initiated-analytics \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:56:20,836 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:56:28,701 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:56:28,771 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:56:29,067 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/telemetry/https%3A/api.gradio.app/gradio-launched-telemetry \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:57:02,175 - __main__ - INFO - Respondiendo pregunta: que sabes sobre la seguridad y la ia \n",
      "2026-01-21 21:57:02,993 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:57:12,299 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:57:57,947 - __main__ - INFO - Respondiendo pregunta: dame una receta de tarta de manzana \n",
      "2026-01-21 21:57:58,534 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:58:08,963 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:58:29,675 - __main__ - INFO - Respondiendo pregunta: ¿Qué recomendaciones se dan para el seguro del hogar?\n",
      "2026-01-21 21:58:30,170 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-21 21:58:44,210 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# ejecutamos el main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
